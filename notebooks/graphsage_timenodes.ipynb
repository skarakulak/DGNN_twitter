{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, load_data\n",
    "from dgl.data import BitcoinOTC\n",
    "import datetime\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import networkx as nx\n",
    "from dgl import DGLGraph\n",
    "from dgl.nn.pytorch.conv import SAGEConv\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 100386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_type = 'mean' #mean/gcn/pool/lstm\n",
    "hid_dim = 128\n",
    "n_layers = 2\n",
    "dropout = 0\n",
    "learning_rate = 0.0003\n",
    "wt_decay = 0\n",
    "stpsize = 120\n",
    "checkpt_iter = 5\n",
    "n_epochs = 100\n",
    "out_path = '/misc/vlgscratch4/BrunaGroup/rj1408/dynamic_nn/models/twitter/timenodes/'\n",
    "data_path = '../twitter_data/'\n",
    "activation = F.leaky_relu\n",
    "num_new_nodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hate(features, edges, num_features):\n",
    "    num_nodes = 100386\n",
    "    num_feats = num_features\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "\n",
    "    with open(features) as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            feat_data[i, :] = list(map(float, info[1:-1]))\n",
    "            node_map[info[0]] = i\n",
    "            if not info[-1] in label_map:\n",
    "                label_map[info[-1]] = len(label_map)\n",
    "            labels[i] = label_map[info[-1]]\n",
    "\n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(edges) as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "\n",
    "    print(label_map)\n",
    "    temp = [[[k]*len(v), list(v)] for k,v in adj_lists.items()]\n",
    "    temp2 = list(zip(*temp))\n",
    "    src = list(itertools.chain.from_iterable(temp2[0]))\n",
    "    dst = list(itertools.chain.from_iterable(temp2[1]))\n",
    "    return torch.tensor(feat_data).float(), torch.tensor(labels).int().flatten(), (src, dst), node_map, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphs(feat_data, adj_lists, num_new_nodes, only_tweets, origin_to_anom, node_map):\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(feat_data.shape[0])\n",
    "    g.add_edges(adj_lists[0], adj_lists[1])\n",
    "\n",
    "    N = len(only_tweets)\n",
    "    k = int(N/num_new_nodes)\n",
    "    \n",
    "    g.add_nodes(num_new_nodes)\n",
    "\n",
    "    for i in range(num_new_nodes):\n",
    "        slice = only_tweets[i*k:(i+1)*k]\n",
    "        for j in slice.user_id:\n",
    "            g.add_edge(node_map[str(origin_to_anom[j])],i+feat_data.shape[0])\n",
    "            g.add_edge(i+feat_data.shape[0], node_map[str(origin_to_anom[j])])\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normal': 0, 'other': 1, 'hateful': 2}\n"
     ]
    }
   ],
   "source": [
    "feat_data, labels, adj_lists, node_map, label_map = load_hate(os.path.join(data_path, 'public/hate/users_hate_all.content'), os.path.join(data_path, 'public/hate/users.edges'), 320)\n",
    "dd = pd.read_csv(os.path.join(data_path, \"nonpublic/users_neighborhood.csv\"), usecols=['user_id', 'user_id_original'])\n",
    "twits = pd.read_csv(os.path.join(data_path, 'nonpublic/tweets.csv'))\n",
    "\n",
    "anom_to_origin = dict(dd.values)\n",
    "origin_to_anom = {a:b for b,a in anom_to_origin.items()}\n",
    "only_tweets = twits[(twits.rp_flag == False) & (twits.rt_flag == False) & (twits.qt_flag == False)].sort_values(\"tweet_creation\")\n",
    "\n",
    "graph = load_graphs(feat_data, adj_lists, num_new_nodes, only_tweets, origin_to_anom, node_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toprighttens = torch.zeros(feat_data.shape[0], num_new_nodes)\n",
    "bottomlefttens = torch.zeros(num_new_nodes, feat_data.shape[1])\n",
    "bottomrighttens = torch.eye(num_new_nodes)\n",
    "\n",
    "lefttens = torch.cat((feat_data, bottomlefttens), 0)\n",
    "righttens = torch.cat((toprighttens, bottomrighttens), 0)\n",
    "full_feat = torch.cat((lefttens, righttens), 1)\n",
    "\n",
    "labels[labels == 1] = -1\n",
    "labels[labels == 2] = 1\n",
    "timelabels = torch.zeros(num_new_nodes).int()\n",
    "timelabels[:] = -1\n",
    "labels = torch.cat((labels, timelabels))\n",
    "labels = labels.float()\n",
    "\n",
    "graph.ndata['feat'] = full_feat\n",
    "graph.ndata['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml_lxml(graph.to_networkx(), os.path.join(data_path, \"nonpublic/time_nodes/G_time_withFeats.graphml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_idx = (labels != -1).nonzero().numpy().flatten()\n",
    "train_idx, test_idx = train_test_split(annotated_idx, test_size=0.4,)\n",
    "train_mask = torch.zeros(num_nodes + num_new_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes + num_new_nodes, dtype=torch.bool)\n",
    "train_mask[torch.tensor(train_idx)]=True\n",
    "val_mask[torch.tensor(test_idx)]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_normals = (labels==0) * train_mask\n",
    "training_hatefuls = (labels==1) * train_mask\n",
    "ratio_h2n = training_hatefuls.sum().float() / training_normals.sum().float()\n",
    "\n",
    "bernoulli = torch.distributions.bernoulli.Bernoulli(ratio_h2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = (labels== 1).long().sum().item()\n",
    "neg = (labels== 0).long().sum().item()\n",
    "imbalancefac = neg/pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.droplayer = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # input layer\n",
    "        self.inplayer = nn.Linear(in_feats, n_hidden)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        # hidden layers\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type, feat_drop=dropout, activation=activation))\n",
    "        \n",
    "        # output layer\n",
    "        self.outlayer = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, features, graph):\n",
    "        h = features\n",
    "        h = self.inplayer(h)\n",
    "        h = self.activation(h)\n",
    "        h = self.droplayer(h)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(graph, h)\n",
    "            \n",
    "        h = self.outlayer(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logits(model, device, graph, mask=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = graph.ndata['feat'].to(device)\n",
    "        logits = model(features, graph).flatten()\n",
    "        \n",
    "        if mask is not None:\n",
    "            logits = logits[mask]\n",
    "    return logits\n",
    "\n",
    "def evaluate(logits, labels, mask=None): \n",
    "    if mask is not None:\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "    \n",
    "    sigLayer = nn.Sigmoid()\n",
    "    predictions_scores = sigLayer(logits).detach().numpy()\n",
    "    roc_auc = metrics.roc_auc_score(labels, predictions_scores)\n",
    "    \n",
    "    indices = (logits > 0).long()\n",
    "    correct = torch.sum(indices == labels)\n",
    "    return (roc_auc, correct.item() * 1.0 / len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, criterion, device, val_mask, graph):\n",
    "    model.eval()\n",
    "    \n",
    "    #validation phase\n",
    "    with torch.set_grad_enabled(False):\n",
    "        feat = graph.ndata['feat'].to(device)\n",
    "        outputs = model(feat, graph).flatten()\n",
    "        labels = graph.ndata['labels'].to(device)\n",
    "        loss = criterion(outputs[val_mask], labels[val_mask])\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for supervised training\n",
    "def train_model(model, criterion, optimizer, scheduler, device, checkpoint_path, graph, checkpoint_iter, hyperparams, num_epochs=25):\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\"train\"] = {}\n",
    "    metrics_dict[\"valid\"] = {}\n",
    "    metrics_dict[\"train\"][\"loss\"] = {}\n",
    "    metrics_dict[\"train\"][\"loss\"][\"epochwise\"] = []\n",
    "    metrics_dict[\"valid\"][\"loss\"] = {}\n",
    "    metrics_dict[\"valid\"][\"loss\"][\"epochwise\"] = []\n",
    "        \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        und_sampled_normal_idx = training_normals.nonzero()[\n",
    "            bernoulli.sample([training_normals.sum()]).bool()].flatten()\n",
    "\n",
    "        balanced_train_mask = torch.zeros(train_mask.size(0),dtype=torch.bool)\n",
    "        balanced_train_mask[training_hatefuls] = True\n",
    "        balanced_train_mask[und_sampled_normal_idx] = True\n",
    "        \n",
    "        #train phase\n",
    "        scheduler.step()\n",
    "        model.train() \n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        forward_start_time  = time.time()\n",
    "        feats = graph.ndata['feat'].to(device)\n",
    "        outputs = model(feats, graph).flatten()\n",
    "        labels = graph.ndata['labels']\n",
    "        labels = labels.to(device)\n",
    "        loss = criterion(outputs[train_mask], labels[train_mask])\n",
    "        epoch_loss = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        forward_time = time.time() - forward_start_time\n",
    "        \n",
    "        #validation phase\n",
    "        val_epoch_loss = evaluate_loss(model, criterion, device, val_mask, graph)\n",
    "        \n",
    "        metrics_dict[\"train\"][\"loss\"][\"epochwise\"].append(epoch_loss)\n",
    "        metrics_dict[\"valid\"][\"loss\"][\"epochwise\"].append(val_epoch_loss)\n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_epoch_loss < best_loss:\n",
    "            best_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        if epoch%checkpoint_iter==0:\n",
    "            print('Epoch {}/{} \\n'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "            print('\\n')\n",
    "            print('Train Loss: {:.4f} \\n'.format(epoch_loss))\n",
    "            print('Validation Loss: {:.4f} \\n'.format(val_epoch_loss))\n",
    "            \n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'full_metrics': metrics_dict,\n",
    "            'hyperparams': hyperparams\n",
    "            }, '%s/net_epoch_%d.pth' % (checkpoint_path, epoch))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s \\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f} \\n'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rj1408/anaconda3/envs/dgl_env/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 1.2540 \n",
      "\n",
      "Validation Loss: 1.1909 \n",
      "\n",
      "Epoch 5/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 1.0368 \n",
      "\n",
      "Validation Loss: 0.9992 \n",
      "\n",
      "Epoch 10/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.8824 \n",
      "\n",
      "Validation Loss: 0.8635 \n",
      "\n",
      "Epoch 15/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.7756 \n",
      "\n",
      "Validation Loss: 0.7767 \n",
      "\n",
      "Epoch 20/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.7009 \n",
      "\n",
      "Validation Loss: 0.7177 \n",
      "\n",
      "Epoch 25/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.6499 \n",
      "\n",
      "Validation Loss: 0.6883 \n",
      "\n",
      "Epoch 30/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.6186 \n",
      "\n",
      "Validation Loss: 0.6894 \n",
      "\n",
      "Epoch 35/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.5938 \n",
      "\n",
      "Validation Loss: 0.6943 \n",
      "\n",
      "Epoch 40/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.5701 \n",
      "\n",
      "Validation Loss: 0.6956 \n",
      "\n",
      "Epoch 45/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.5469 \n",
      "\n",
      "Validation Loss: 0.7013 \n",
      "\n",
      "Epoch 50/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.5250 \n",
      "\n",
      "Validation Loss: 0.7050 \n",
      "\n",
      "Epoch 55/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.5042 \n",
      "\n",
      "Validation Loss: 0.7120 \n",
      "\n",
      "Epoch 60/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.4834 \n",
      "\n",
      "Validation Loss: 0.7218 \n",
      "\n",
      "Epoch 65/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.4620 \n",
      "\n",
      "Validation Loss: 0.7356 \n",
      "\n",
      "Epoch 70/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.4398 \n",
      "\n",
      "Validation Loss: 0.7515 \n",
      "\n",
      "Epoch 75/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.4170 \n",
      "\n",
      "Validation Loss: 0.7705 \n",
      "\n",
      "Epoch 80/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.3930 \n",
      "\n",
      "Validation Loss: 0.7936 \n",
      "\n",
      "Epoch 85/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.3684 \n",
      "\n",
      "Validation Loss: 0.8202 \n",
      "\n",
      "Epoch 90/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.3430 \n",
      "\n",
      "Validation Loss: 0.8499 \n",
      "\n",
      "Epoch 95/99 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 0.3171 \n",
      "\n",
      "Validation Loss: 0.8840 \n",
      "\n",
      "Training complete in 0m 23s \n",
      "\n",
      "Best val loss: 0.685906 \n",
      "\n"
     ]
    }
   ],
   "source": [
    " # create GCN model\n",
    "model = GraphSAGE(graph.ndata['feat'].shape[1], hid_dim, 1, n_layers, activation, dropout, aggregator_type)\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([imbalancefac]).to(device))\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(model_parameters, lr=learning_rate, weight_decay = wt_decay)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=stpsize, gamma=0.1)\n",
    "hyper_params = {'hid_dim': hid_dim,\n",
    "    'n_layers' : n_layers,\n",
    "    'dropout' : dropout,\n",
    "    'wt_decay' : wt_decay}\n",
    "\n",
    "bst_model = train_model(model, criterion, optimizer, exp_lr_scheduler, device, out_path, graph, checkpt_iter, hyper_params, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = predict_logits(bst_model, device, graph, val_mask)\n",
    "auc, accuracy = evaluate(logits.cpu(), labels[val_mask].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9064104251678294, 0.8501759678230266)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc, accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
